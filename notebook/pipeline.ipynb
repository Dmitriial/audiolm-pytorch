{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f106d8-b9df-42a7-9a7e-a86e9cddb732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svarog/anaconda3/envs/audiolm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "# imports\n",
    "import math\n",
    "import wave\n",
    "import struct\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchaudio\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b70fab-8d1b-49e6-a8e9-7cf26eed3c0b",
   "metadata": {},
   "source": [
    "## Using only clean records from librispeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77146bb3-6122-4cb5-bf5c-08e422e22fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "libtrispeech_path = Path(\"../librispeech_finetuning\")\n",
    "assert libtrispeech_path.exists(), \"Must be exists!\"\n",
    "\n",
    "marker = \"clean\"\n",
    "destination_dir = Path(f\"../librispeech_{marker}_flac\")\n",
    "if not destination_dir.exists():\n",
    "    destination_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db617c9f-1a41-4c4e-99e9-8c6a2b3792ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2763it [00:37, 74.66it/s]  \n"
     ]
    }
   ],
   "source": [
    "for flac in tqdm(libtrispeech_path.glob(\"**/*.flac\")):\n",
    "    if marker in str(flac):\n",
    "        shutil.copy(flac, Path(destination_dir, flac.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e19dac-d680-4823-b3bd-746b21054bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert flac to wavs (!)\n",
    "# !pip install AudioConverter\n",
    "# !audioconvert convert  vall-e/data/libri --output-format .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c620785f-9a12-4e55-992b-622565585c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_wavs_dir = Path(\"../librispeech_wavs\")\n",
    "assert destination_wavs_dir.exists(), \"Must be exists!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cb5286f-a762-47b2-a473-71c8f8fa148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all dataset paths, checkpoints, etc\n",
    "dataset_folder = str(destination_wavs_dir)\n",
    "\n",
    "# soundstream_ckpt = \"results/soundstream.8.pt\" # this can change depending on number of steps\n",
    "# hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
    "# hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer\n",
    "\n",
    "# soundstream_ckpt = \"results/soundstream.8.pt\" # this can change depending on number of steps\n",
    "soundstream_ckpt = \"results/soundstream.0.pt\" # this can change depending on number of steps\n",
    "\n",
    "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
    "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' \n",
    "# listed in row \"HuBERT Base (~95M params)\", column Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9de7616-d8c2-423d-8d86-9a2c17ba87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder data generation\n",
    "def get_sinewave(freq=440.0, duration_ms=200, volume=1.0, sample_rate=44100.0):\n",
    "  # code adapted from https://stackoverflow.com/a/33913403\n",
    "  audio = []\n",
    "  num_samples = duration_ms * (sample_rate / 1000.0)\n",
    "  for x in range(int(num_samples)):\n",
    "    audio.append(volume * math.sin(2 * math.pi * freq * (x / sample_rate)))\n",
    "  return audio\n",
    "\n",
    "def save_wav(file_name, audio, sample_rate=44100.0):\n",
    "  # Open up a wav file\n",
    "  wav_file=wave.open(file_name,\"w\")\n",
    "  # wav params\n",
    "  nchannels = 1\n",
    "  sampwidth = 2\n",
    "  # 44100 is the industry standard sample rate - CD quality.  If you need to\n",
    "  # save on file size you can adjust it downwards. The stanard for low quality\n",
    "  # is 8000 or 8kHz.\n",
    "  nframes = len(audio)\n",
    "  comptype = \"NONE\"\n",
    "  compname = \"not compressed\"\n",
    "  wav_file.setparams((nchannels, sampwidth, sample_rate, nframes, comptype, compname))\n",
    "  # WAV files here are using short, 16 bit, signed integers for the \n",
    "  # sample size.  So we multiply the floating point data we have by 32767, the\n",
    "  # maximum value for a short integer.  NOTE: It is theortically possible to\n",
    "  # use the floating point -1.0 to 1.0 data directly in a WAV file but not\n",
    "  # obvious how to do that using the wave module in python.\n",
    "  for sample in audio:\n",
    "      wav_file.writeframes(struct.pack('h', int( sample * 32767.0 )))\n",
    "  wav_file.close()\n",
    "  return\n",
    "\n",
    "def make_placeholder_dataset():\n",
    "  # Make a placeholder dataset with a few .wav files that you can \"train\" on, just to verify things work e2e\n",
    "  if os.path.isdir(dataset_folder):\n",
    "    return\n",
    "  os.makedirs(dataset_folder)\n",
    "  save_wav(f\"{dataset_folder}/example.wav\", get_sinewave())\n",
    "  save_wav(f\"{dataset_folder}/example2.wav\", get_sinewave(duration_ms=500))\n",
    "  os.makedirs(f\"{dataset_folder}/subdirectory\")\n",
    "  save_wav(f\"{dataset_folder}/subdirectory/example.wav\", get_sinewave(freq=330.0))\n",
    "\n",
    "make_placeholder_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95f2eb9-7315-458c-86e9-3ac0f226366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual dataset. Uncomment this if you want to try training on real data\n",
    "\n",
    "# full dataset: https://www.openslr.org/12\n",
    "# We'll use https://us.openslr.org/resources/12/dev-clean.tar.gz development set, \"clean\" speech.\n",
    "# We *should* train on, well, training, but this is just to demo running things end-to-end at all so I just picked a small clean set.\n",
    "\n",
    "# url = \"https://us.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "# filename = \"dev-clean\"\n",
    "# filename_targz = filename + \".tar.gz\"\n",
    "# if not os.path.isfile(filename_targz):\n",
    "#   urllib.request.urlretrieve(url, filename_targz)\n",
    "# if not os.path.isdir(filename):\n",
    "#   # open file\n",
    "#   with tarfile.open(filename_targz) as t:\n",
    "#     t.extractall(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61418b-73a2-4ff3-93b6-1316056edfad",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have a dataset, we can train AudioLM.\n",
    "\n",
    "**Note**: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3409bcb-07f7-4cb4-b910-6a7e5a55c222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 1295 samples and validating with randomly splitted 69 samples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want to clear previous experiment checkpoints and results? (y/n)  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: soundstream total loss: 29.295, soundstream recon loss: 0.019 | discr (scale 1) loss: 2.000 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 2.000\n",
      "0: saving to results\n",
      "0: saving model to results\n",
      "1: soundstream total loss: 23.763, soundstream recon loss: 0.008 | discr (scale 1) loss: 2.001 | discr (scale 0.5) loss: 2.001 | discr (scale 0.25) loss: 2.000\n",
      "2: soundstream total loss: 22.886, soundstream recon loss: 0.004 | discr (scale 1) loss: 1.999 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 2.000\n",
      "2: saving to results\n",
      "3: soundstream total loss: 34.897, soundstream recon loss: 0.011 | discr (scale 1) loss: 1.998 | discr (scale 0.5) loss: 1.999 | discr (scale 0.25) loss: 1.999\n",
      "4: soundstream total loss: 23.379, soundstream recon loss: 0.006 | discr (scale 1) loss: 1.998 | discr (scale 0.5) loss: 1.999 | discr (scale 0.25) loss: 1.999\n",
      "4: saving to results\n",
      "4: saving model to results\n",
      "5: soundstream total loss: 22.289, soundstream recon loss: 0.004 | discr (scale 1) loss: 1.999 | discr (scale 0.5) loss: 1.999 | discr (scale 0.25) loss: 1.999\n",
      "6: soundstream total loss: 24.811, soundstream recon loss: 0.006 | discr (scale 1) loss: 2.004 | discr (scale 0.5) loss: 2.001 | discr (scale 0.25) loss: 2.000\n",
      "6: saving to results\n",
      "7: soundstream total loss: 18.589, soundstream recon loss: 0.004 | discr (scale 1) loss: 2.004 | discr (scale 0.5) loss: 2.001 | discr (scale 0.25) loss: 2.001\n",
      "8: soundstream total loss: 20.780, soundstream recon loss: 0.005 | discr (scale 1) loss: 2.004 | discr (scale 0.5) loss: 2.001 | discr (scale 0.25) loss: 2.001\n",
      "8: saving to results\n",
      "8: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 2,  # effective batch size of 32\n",
    "    grad_accum_every = 8,         \n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ").cuda()\n",
    "\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebe2c2f-7cab-433f-8318-6f450416ac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 605/605 [00:00<00:00, 3.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 1295 samples and validating with randomly splitted 69 samples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want to clear previous experiment checkpoints and results? (y/n)  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing label assignment and total inertia\n",
      "0: loss: 6.317939281463623\n",
      "Computing label assignment and total inertia\n",
      "0: valid loss 6.484222888946533\n",
      "0: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# hubert checkpoints can be downloaded at\n",
    "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
    "if not os.path.isdir(\"hubert\"):\n",
    "  os.makedirs(\"hubert\")\n",
    "if not os.path.isfile(hubert_ckpt):\n",
    "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
    "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
    "if not os.path.isfile(hubert_quantizer):\n",
    "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
    "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "semantic_transformer = SemanticTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    dim = 1024,\n",
    "    depth = 6\n",
    ").cuda()\n",
    "\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54bd649a-b5eb-491f-b852-6aeaf00e3db3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SoundStream:\n\tMissing key(s) in state_dict: \"encoder.0.conv.weight\", \"encoder.0.conv.bias\", \"encoder.1.0.fn.0.conv.weight\", \"encoder.1.0.fn.0.conv.bias\", \"encoder.1.0.fn.2.conv.weight\", \"encoder.1.0.fn.2.conv.bias\", \"encoder.1.1.fn.0.conv.weight\", \"encoder.1.1.fn.0.conv.bias\", \"encoder.1.1.fn.2.conv.weight\", \"encoder.1.1.fn.2.conv.bias\", \"encoder.1.2.fn.0.conv.weight\", \"encoder.1.2.fn.0.conv.bias\", \"encoder.1.2.fn.2.conv.weight\", \"encoder.1.2.fn.2.conv.bias\", \"encoder.1.3.conv.weight\", \"encoder.1.3.conv.bias\", \"encoder.2.0.fn.0.conv.weight\", \"encoder.2.0.fn.0.conv.bias\", \"encoder.2.0.fn.2.conv.weight\", \"encoder.2.0.fn.2.conv.bias\", \"encoder.2.1.fn.0.conv.weight\", \"encoder.2.1.fn.0.conv.bias\", \"encoder.2.1.fn.2.conv.weight\", \"encoder.2.1.fn.2.conv.bias\", \"encoder.2.2.fn.0.conv.weight\", \"encoder.2.2.fn.0.conv.bias\", \"encoder.2.2.fn.2.conv.weight\", \"encoder.2.2.fn.2.conv.bias\", \"encoder.2.3.conv.weight\", \"encoder.2.3.conv.bias\", \"encoder.3.0.fn.0.conv.weight\", \"encoder.3.0.fn.0.conv.bias\", \"encoder.3.0.fn.2.conv.weight\", \"encoder.3.0.fn.2.conv.bias\", \"encoder.3.1.fn.0.conv.weight\", \"encoder.3.1.fn.0.conv.bias\", \"encoder.3.1.fn.2.conv.weight\", \"encoder.3.1.fn.2.conv.bias\", \"encoder.3.2.fn.0.conv.weight\", \"encoder.3.2.fn.0.conv.bias\", \"encoder.3.2.fn.2.conv.weight\", \"encoder.3.2.fn.2.conv.bias\", \"encoder.3.3.conv.weight\", \"encoder.3.3.conv.bias\", \"encoder.4.0.fn.0.conv.weight\", \"encoder.4.0.fn.0.conv.bias\", \"encoder.4.0.fn.2.conv.weight\", \"encoder.4.0.fn.2.conv.bias\", \"encoder.4.1.fn.0.conv.weight\", \"encoder.4.1.fn.0.conv.bias\", \"encoder.4.1.fn.2.conv.weight\", \"encoder.4.1.fn.2.conv.bias\", \"encoder.4.2.fn.0.conv.weight\", \"encoder.4.2.fn.0.conv.bias\", \"encoder.4.2.fn.2.conv.weight\", \"encoder.4.2.fn.2.conv.bias\", \"encoder.4.3.conv.weight\", \"encoder.4.3.conv.bias\", \"encoder.5.conv.weight\", \"encoder.5.conv.bias\", \"encoder_attn.layers.0.0.q_scale\", \"encoder_attn.layers.0.0.k_scale\", \"encoder_attn.layers.0.0.norm.weight\", \"encoder_attn.layers.0.0.norm.bias\", \"encoder_attn.layers.0.0.to_qkv.weight\", \"encoder_attn.layers.0.0.attn_fn.rel_pos.inv_freq\", \"encoder_attn.layers.0.0.to_out.weight\", \"encoder_attn.layers.0.1.0.weight\", \"encoder_attn.layers.0.1.0.bias\", \"encoder_attn.layers.0.1.1.weight\", \"encoder_attn.layers.0.1.4.weight\", \"encoder_film.to_cond.weight\", \"encoder_film.to_cond.bias\", \"rq.rvqs.0.layers.0._codebook.initted\", \"rq.rvqs.0.layers.0._codebook.cluster_size\", \"rq.rvqs.0.layers.0._codebook.embed_avg\", \"rq.rvqs.0.layers.0._codebook.embed\", \"rq.rvqs.0.layers.1._codebook.initted\", \"rq.rvqs.0.layers.1._codebook.cluster_size\", \"rq.rvqs.0.layers.1._codebook.embed_avg\", \"rq.rvqs.0.layers.1._codebook.embed\", \"rq.rvqs.0.layers.2._codebook.initted\", \"rq.rvqs.0.layers.2._codebook.cluster_size\", \"rq.rvqs.0.layers.2._codebook.embed_avg\", \"rq.rvqs.0.layers.2._codebook.embed\", \"rq.rvqs.0.layers.3._codebook.initted\", \"rq.rvqs.0.layers.3._codebook.cluster_size\", \"rq.rvqs.0.layers.3._codebook.embed_avg\", \"rq.rvqs.0.layers.3._codebook.embed\", \"rq.rvqs.0.layers.4._codebook.initted\", \"rq.rvqs.0.layers.4._codebook.cluster_size\", \"rq.rvqs.0.layers.4._codebook.embed_avg\", \"rq.rvqs.0.layers.4._codebook.embed\", \"rq.rvqs.0.layers.5._codebook.initted\", \"rq.rvqs.0.layers.5._codebook.cluster_size\", \"rq.rvqs.0.layers.5._codebook.embed_avg\", \"rq.rvqs.0.layers.5._codebook.embed\", \"rq.rvqs.0.layers.6._codebook.initted\", \"rq.rvqs.0.layers.6._codebook.cluster_size\", \"rq.rvqs.0.layers.6._codebook.embed_avg\", \"rq.rvqs.0.layers.6._codebook.embed\", \"rq.rvqs.0.layers.7._codebook.initted\", \"rq.rvqs.0.layers.7._codebook.cluster_size\", \"rq.rvqs.0.layers.7._codebook.embed_avg\", \"rq.rvqs.0.layers.7._codebook.embed\", \"decoder_film.to_cond.weight\", \"decoder_film.to_cond.bias\", \"decoder_attn.layers.0.0.q_scale\", \"decoder_attn.layers.0.0.k_scale\", \"decoder_attn.layers.0.0.norm.weight\", \"decoder_attn.layers.0.0.norm.bias\", \"decoder_attn.layers.0.0.to_qkv.weight\", \"decoder_attn.layers.0.0.attn_fn.rel_pos.inv_freq\", \"decoder_attn.layers.0.0.to_out.weight\", \"decoder_attn.layers.0.1.0.weight\", \"decoder_attn.layers.0.1.0.bias\", \"decoder_attn.layers.0.1.1.weight\", \"decoder_attn.layers.0.1.4.weight\", \"decoder.0.conv.weight\", \"decoder.0.conv.bias\", \"decoder.1.0.conv.weight\", \"decoder.1.0.conv.bias\", \"decoder.1.1.fn.0.conv.weight\", \"decoder.1.1.fn.0.conv.bias\", \"decoder.1.1.fn.2.conv.weight\", \"decoder.1.1.fn.2.conv.bias\", \"decoder.1.2.fn.0.conv.weight\", \"decoder.1.2.fn.0.conv.bias\", \"decoder.1.2.fn.2.conv.weight\", \"decoder.1.2.fn.2.conv.bias\", \"decoder.1.3.fn.0.conv.weight\", \"decoder.1.3.fn.0.conv.bias\", \"decoder.1.3.fn.2.conv.weight\", \"decoder.1.3.fn.2.conv.bias\", \"decoder.2.0.conv.weight\", \"decoder.2.0.conv.bias\", \"decoder.2.1.fn.0.conv.weight\", \"decoder.2.1.fn.0.conv.bias\", \"decoder.2.1.fn.2.conv.weight\", \"decoder.2.1.fn.2.conv.bias\", \"decoder.2.2.fn.0.conv.weight\", \"decoder.2.2.fn.0.conv.bias\", \"decoder.2.2.fn.2.conv.weight\", \"decoder.2.2.fn.2.conv.bias\", \"decoder.2.3.fn.0.conv.weight\", \"decoder.2.3.fn.0.conv.bias\", \"decoder.2.3.fn.2.conv.weight\", \"decoder.2.3.fn.2.conv.bias\", \"decoder.3.0.conv.weight\", \"decoder.3.0.conv.bias\", \"decoder.3.1.fn.0.conv.weight\", \"decoder.3.1.fn.0.conv.bias\", \"decoder.3.1.fn.2.conv.weight\", \"decoder.3.1.fn.2.conv.bias\", \"decoder.3.2.fn.0.conv.weight\", \"decoder.3.2.fn.0.conv.bias\", \"decoder.3.2.fn.2.conv.weight\", \"decoder.3.2.fn.2.conv.bias\", \"decoder.3.3.fn.0.conv.weight\", \"decoder.3.3.fn.0.conv.bias\", \"decoder.3.3.fn.2.conv.weight\", \"decoder.3.3.fn.2.conv.bias\", \"decoder.4.0.conv.weight\", \"decoder.4.0.conv.bias\", \"decoder.4.1.fn.0.conv.weight\", \"decoder.4.1.fn.0.conv.bias\", \"decoder.4.1.fn.2.conv.weight\", \"decoder.4.1.fn.2.conv.bias\", \"decoder.4.2.fn.0.conv.weight\", \"decoder.4.2.fn.0.conv.bias\", \"decoder.4.2.fn.2.conv.weight\", \"decoder.4.2.fn.2.conv.bias\", \"decoder.4.3.fn.0.conv.weight\", \"decoder.4.3.fn.0.conv.bias\", \"decoder.4.3.fn.2.conv.weight\", \"decoder.4.3.fn.2.conv.bias\", \"decoder.5.conv.weight\", \"decoder.5.conv.bias\", \"discriminators.0.init_conv.weight\", \"discriminators.0.init_conv.bias\", \"discriminators.0.conv_layers.0.0.weight\", \"discriminators.0.conv_layers.0.0.bias\", \"discriminators.0.conv_layers.1.0.weight\", \"discriminators.0.conv_layers.1.0.bias\", \"discriminators.0.conv_layers.2.0.weight\", \"discriminators.0.conv_layers.2.0.bias\", \"discriminators.0.conv_layers.3.0.weight\", \"discriminators.0.conv_layers.3.0.bias\", \"discriminators.0.final_conv.0.weight\", \"discriminators.0.final_conv.0.bias\", \"discriminators.0.final_conv.2.weight\", \"discriminators.0.final_conv.2.bias\", \"discriminators.1.init_conv.weight\", \"discriminators.1.init_conv.bias\", \"discriminators.1.conv_layers.0.0.weight\", \"discriminators.1.conv_layers.0.0.bias\", \"discriminators.1.conv_layers.1.0.weight\", \"discriminators.1.conv_layers.1.0.bias\", \"discriminators.1.conv_layers.2.0.weight\", \"discriminators.1.conv_layers.2.0.bias\", \"discriminators.1.conv_layers.3.0.weight\", \"discriminators.1.conv_layers.3.0.bias\", \"discriminators.1.final_conv.0.weight\", \"discriminators.1.final_conv.0.bias\", \"discriminators.1.final_conv.2.weight\", \"discriminators.1.final_conv.2.bias\", \"discriminators.2.init_conv.weight\", \"discriminators.2.init_conv.bias\", \"discriminators.2.conv_layers.0.0.weight\", \"discriminators.2.conv_layers.0.0.bias\", \"discriminators.2.conv_layers.1.0.weight\", \"discriminators.2.conv_layers.1.0.bias\", \"discriminators.2.conv_layers.2.0.weight\", \"discriminators.2.conv_layers.2.0.bias\", \"discriminators.2.conv_layers.3.0.weight\", \"discriminators.2.conv_layers.3.0.bias\", \"discriminators.2.final_conv.0.weight\", \"discriminators.2.final_conv.0.bias\", \"discriminators.2.final_conv.2.weight\", \"discriminators.2.final_conv.2.bias\", \"stft_discriminator.init_conv.weight\", \"stft_discriminator.init_conv.bias\", \"stft_discriminator.layers.0.0.fn.0.weight\", \"stft_discriminator.layers.0.0.fn.0.bias\", \"stft_discriminator.layers.0.0.fn.1.b\", \"stft_discriminator.layers.0.0.fn.2.weight\", \"stft_discriminator.layers.0.0.fn.2.bias\", \"stft_discriminator.layers.0.1.weight\", \"stft_discriminator.layers.0.1.bias\", \"stft_discriminator.layers.1.0.fn.0.weight\", \"stft_discriminator.layers.1.0.fn.0.bias\", \"stft_discriminator.layers.1.0.fn.1.b\", \"stft_discriminator.layers.1.0.fn.2.weight\", \"stft_discriminator.layers.1.0.fn.2.bias\", \"stft_discriminator.layers.1.1.weight\", \"stft_discriminator.layers.1.1.bias\", \"stft_discriminator.layers.2.0.fn.0.weight\", \"stft_discriminator.layers.2.0.fn.0.bias\", \"stft_discriminator.layers.2.0.fn.1.b\", \"stft_discriminator.layers.2.0.fn.2.weight\", \"stft_discriminator.layers.2.0.fn.2.bias\", \"stft_discriminator.layers.2.1.weight\", \"stft_discriminator.layers.2.1.bias\", \"stft_discriminator.layers.3.0.fn.0.weight\", \"stft_discriminator.layers.3.0.fn.0.bias\", \"stft_discriminator.layers.3.0.fn.1.b\", \"stft_discriminator.layers.3.0.fn.2.weight\", \"stft_discriminator.layers.3.0.fn.2.bias\", \"stft_discriminator.layers.3.1.weight\", \"stft_discriminator.layers.3.1.bias\", \"stft_discriminator.layers.4.0.fn.0.weight\", \"stft_discriminator.layers.4.0.fn.0.bias\", \"stft_discriminator.layers.4.0.fn.1.b\", \"stft_discriminator.layers.4.0.fn.2.weight\", \"stft_discriminator.layers.4.0.fn.2.bias\", \"stft_discriminator.layers.4.1.weight\", \"stft_discriminator.layers.4.1.bias\", \"stft_discriminator.layers.5.0.fn.0.weight\", \"stft_discriminator.layers.5.0.fn.0.bias\", \"stft_discriminator.layers.5.0.fn.1.b\", \"stft_discriminator.layers.5.0.fn.2.weight\", \"stft_discriminator.layers.5.0.fn.2.bias\", \"stft_discriminator.layers.5.1.weight\", \"stft_discriminator.layers.5.1.bias\", \"stft_discriminator.final_conv.weight\", \"stft_discriminator.final_conv.bias\", \"mel_spec_transforms.0.spectrogram.window\", \"mel_spec_transforms.0.mel_scale.fb\", \"mel_spec_transforms.1.spectrogram.window\", \"mel_spec_transforms.1.mel_scale.fb\", \"mel_spec_transforms.2.spectrogram.window\", \"mel_spec_transforms.2.mel_scale.fb\", \"mel_spec_transforms.3.spectrogram.window\", \"mel_spec_transforms.3.mel_scale.fb\", \"mel_spec_transforms.4.spectrogram.window\", \"mel_spec_transforms.4.mel_scale.fb\", \"mel_spec_transforms.5.spectrogram.window\", \"mel_spec_transforms.5.mel_scale.fb\". \n\tUnexpected key(s) in state_dict: \"start_token\", \"semantic_embedding.weight\", \"proj_text_embed.weight\", \"transformer.layers.0.0.norm.gamma\", \"transformer.layers.0.0.norm.beta\", \"transformer.layers.0.0.to_q.weight\", \"transformer.layers.0.0.to_kv.weight\", \"transformer.layers.0.0.to_out.0.weight\", \"transformer.layers.0.2.0.gamma\", \"transformer.layers.0.2.0.beta\", \"transformer.layers.0.2.1.weight\", \"transformer.layers.0.2.3.gamma\", \"transformer.layers.0.2.3.beta\", \"transformer.layers.0.2.5.weight\", \"transformer.layers.1.0.norm.gamma\", \"transformer.layers.1.0.norm.beta\", \"transformer.layers.1.0.to_q.weight\", \"transformer.layers.1.0.to_kv.weight\", \"transformer.layers.1.0.to_out.0.weight\", \"transformer.layers.1.2.0.gamma\", \"transformer.layers.1.2.0.beta\", \"transformer.layers.1.2.1.weight\", \"transformer.layers.1.2.3.gamma\", \"transformer.layers.1.2.3.beta\", \"transformer.layers.1.2.5.weight\", \"transformer.layers.2.0.norm.gamma\", \"transformer.layers.2.0.norm.beta\", \"transformer.layers.2.0.to_q.weight\", \"transformer.layers.2.0.to_kv.weight\", \"transformer.layers.2.0.to_out.0.weight\", \"transformer.layers.2.2.0.gamma\", \"transformer.layers.2.2.0.beta\", \"transformer.layers.2.2.1.weight\", \"transformer.layers.2.2.3.gamma\", \"transformer.layers.2.2.3.beta\", \"transformer.layers.2.2.5.weight\", \"transformer.layers.3.0.norm.gamma\", \"transformer.layers.3.0.norm.beta\", \"transformer.layers.3.0.to_q.weight\", \"transformer.layers.3.0.to_kv.weight\", \"transformer.layers.3.0.to_out.0.weight\", \"transformer.layers.3.2.0.gamma\", \"transformer.layers.3.2.0.beta\", \"transformer.layers.3.2.1.weight\", \"transformer.layers.3.2.3.gamma\", \"transformer.layers.3.2.3.beta\", \"transformer.layers.3.2.5.weight\", \"transformer.layers.4.0.norm.gamma\", \"transformer.layers.4.0.norm.beta\", \"transformer.layers.4.0.to_q.weight\", \"transformer.layers.4.0.to_kv.weight\", \"transformer.layers.4.0.to_out.0.weight\", \"transformer.layers.4.2.0.gamma\", \"transformer.layers.4.2.0.beta\", \"transformer.layers.4.2.1.weight\", \"transformer.layers.4.2.3.gamma\", \"transformer.layers.4.2.3.beta\", \"transformer.layers.4.2.5.weight\", \"transformer.layers.5.0.norm.gamma\", \"transformer.layers.5.0.norm.beta\", \"transformer.layers.5.0.to_q.weight\", \"transformer.layers.5.0.to_kv.weight\", \"transformer.layers.5.0.to_out.0.weight\", \"transformer.layers.5.2.0.gamma\", \"transformer.layers.5.2.0.beta\", \"transformer.layers.5.2.1.weight\", \"transformer.layers.5.2.3.gamma\", \"transformer.layers.5.2.3.beta\", \"transformer.layers.5.2.5.weight\", \"transformer.rel_pos_bias.net.0.0.weight\", \"transformer.rel_pos_bias.net.0.0.bias\", \"transformer.rel_pos_bias.net.1.0.weight\", \"transformer.rel_pos_bias.net.1.0.bias\", \"transformer.rel_pos_bias.net.2.0.weight\", \"transformer.rel_pos_bias.net.2.0.bias\", \"transformer.rel_pos_bias.net.3.weight\", \"transformer.rel_pos_bias.net.3.bias\", \"transformer.norm.gamma\", \"transformer.norm.beta\", \"to_logits.weight\", \"to_logits.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m soundstream_ckpt \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/semantic.transformer.0.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m soundstream_ckpt\u001b[38;5;241m.\u001b[39mexists()\n\u001b[0;32m---> 14\u001b[0m \u001b[43msoundstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msoundstream_ckpt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m coarse_transformer \u001b[38;5;241m=\u001b[39m CoarseTransformer(\n\u001b[1;32m     17\u001b[0m     num_semantic_tokens \u001b[38;5;241m=\u001b[39m wav2vec\u001b[38;5;241m.\u001b[39mcodebook_size,\n\u001b[1;32m     18\u001b[0m     codebook_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CoarseTransformerTrainer(\n\u001b[1;32m     25\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m coarse_transformer,\n\u001b[1;32m     26\u001b[0m     codec \u001b[38;5;241m=\u001b[39m soundstream,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     num_train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/py/audiolm-pytorch/audiolm_pytorch/soundstream.py:664\u001b[0m, in \u001b[0;36mSoundStream.load\u001b[0;34m(self, path, strict)\u001b[0m\n\u001b[1;32m    661\u001b[0m     model_pkg \u001b[38;5;241m=\u001b[39m filter_by_keys(\u001b[38;5;28;01mlambda\u001b[39;00m k: k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema_model.\u001b[39m\u001b[38;5;124m'\u001b[39m), model_pkg)\n\u001b[1;32m    662\u001b[0m     model_pkg \u001b[38;5;241m=\u001b[39m map_keys(\u001b[38;5;28;01mlambda\u001b[39;00m k: k[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema_model.\u001b[39m\u001b[38;5;124m'\u001b[39m):], model_pkg)\n\u001b[0;32m--> 664\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audiolm/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SoundStream:\n\tMissing key(s) in state_dict: \"encoder.0.conv.weight\", \"encoder.0.conv.bias\", \"encoder.1.0.fn.0.conv.weight\", \"encoder.1.0.fn.0.conv.bias\", \"encoder.1.0.fn.2.conv.weight\", \"encoder.1.0.fn.2.conv.bias\", \"encoder.1.1.fn.0.conv.weight\", \"encoder.1.1.fn.0.conv.bias\", \"encoder.1.1.fn.2.conv.weight\", \"encoder.1.1.fn.2.conv.bias\", \"encoder.1.2.fn.0.conv.weight\", \"encoder.1.2.fn.0.conv.bias\", \"encoder.1.2.fn.2.conv.weight\", \"encoder.1.2.fn.2.conv.bias\", \"encoder.1.3.conv.weight\", \"encoder.1.3.conv.bias\", \"encoder.2.0.fn.0.conv.weight\", \"encoder.2.0.fn.0.conv.bias\", \"encoder.2.0.fn.2.conv.weight\", \"encoder.2.0.fn.2.conv.bias\", \"encoder.2.1.fn.0.conv.weight\", \"encoder.2.1.fn.0.conv.bias\", \"encoder.2.1.fn.2.conv.weight\", \"encoder.2.1.fn.2.conv.bias\", \"encoder.2.2.fn.0.conv.weight\", \"encoder.2.2.fn.0.conv.bias\", \"encoder.2.2.fn.2.conv.weight\", \"encoder.2.2.fn.2.conv.bias\", \"encoder.2.3.conv.weight\", \"encoder.2.3.conv.bias\", \"encoder.3.0.fn.0.conv.weight\", \"encoder.3.0.fn.0.conv.bias\", \"encoder.3.0.fn.2.conv.weight\", \"encoder.3.0.fn.2.conv.bias\", \"encoder.3.1.fn.0.conv.weight\", \"encoder.3.1.fn.0.conv.bias\", \"encoder.3.1.fn.2.conv.weight\", \"encoder.3.1.fn.2.conv.bias\", \"encoder.3.2.fn.0.conv.weight\", \"encoder.3.2.fn.0.conv.bias\", \"encoder.3.2.fn.2.conv.weight\", \"encoder.3.2.fn.2.conv.bias\", \"encoder.3.3.conv.weight\", \"encoder.3.3.conv.bias\", \"encoder.4.0.fn.0.conv.weight\", \"encoder.4.0.fn.0.conv.bias\", \"encoder.4.0.fn.2.conv.weight\", \"encoder.4.0.fn.2.conv.bias\", \"encoder.4.1.fn.0.conv.weight\", \"encoder.4.1.fn.0.conv.bias\", \"encoder.4.1.fn.2.conv.weight\", \"encoder.4.1.fn.2.conv.bias\", \"encoder.4.2.fn.0.conv.weight\", \"encoder.4.2.fn.0.conv.bias\", \"encoder.4.2.fn.2.conv.weight\", \"encoder.4.2.fn.2.conv.bias\", \"encoder.4.3.conv.weight\", \"encoder.4.3.conv.bias\", \"encoder.5.conv.weight\", \"encoder.5.conv.bias\", \"encoder_attn.layers.0.0.q_scale\", \"encoder_attn.layers.0.0.k_scale\", \"encoder_attn.layers.0.0.norm.weight\", \"encoder_attn.layers.0.0.norm.bias\", \"encoder_attn.layers.0.0.to_qkv.weight\", \"encoder_attn.layers.0.0.attn_fn.rel_pos.inv_freq\", \"encoder_attn.layers.0.0.to_out.weight\", \"encoder_attn.layers.0.1.0.weight\", \"encoder_attn.layers.0.1.0.bias\", \"encoder_attn.layers.0.1.1.weight\", \"encoder_attn.layers.0.1.4.weight\", \"encoder_film.to_cond.weight\", \"encoder_film.to_cond.bias\", \"rq.rvqs.0.layers.0._codebook.initted\", \"rq.rvqs.0.layers.0._codebook.cluster_size\", \"rq.rvqs.0.layers.0._codebook.embed_avg\", \"rq.rvqs.0.layers.0._codebook.embed\", \"rq.rvqs.0.layers.1._codebook.initted\", \"rq.rvqs.0.layers.1._codebook.cluster_size\", \"rq.rvqs.0.layers.1._codebook.embed_avg\", \"rq.rvqs.0.layers.1._codebook.embed\", \"rq.rvqs.0.layers.2._codebook.initted\", \"rq.rvqs.0.layers.2._codebook.cluster_size\", \"rq.rvqs.0.layers.2._codebook.embed_avg\", \"rq.rvqs.0.layers.2._codebook.embed\", \"rq.rvqs.0.layers.3._codebook.initted\", \"rq.rvqs.0.layers.3._codebook.cluster_size\", \"rq.rvqs.0.layers.3._codebook.embed_avg\", \"rq.rvqs.0.layers.3._codebook.embed\", \"rq.rvqs.0.layers.4._codebook.initted\", \"rq.rvqs.0.layers.4._codebook.cluster_size\", \"rq.rvqs.0.layers.4._codebook.embed_avg\", \"rq.rvqs.0.layers.4._codebook.embed\", \"rq.rvqs.0.layers.5._codebook.initted\", \"rq.rvqs.0.layers.5._codebook.cluster_size\", \"rq.rvqs.0.layers.5._codebook.embed_avg\", \"rq.rvqs.0.layers.5._codebook.embed\", \"rq.rvqs.0.layers.6._codebook.initted\", \"rq.rvqs.0.layers.6._codebook.cluster_size\", \"rq.rvqs.0.layers.6._codebook.embed_avg\", \"rq.rvqs.0.layers.6._codebook.embed\", \"rq.rvqs.0.layers.7._codebook.initted\", \"rq.rvqs.0.layers.7._codebook.cluster_size\", \"rq.rvqs.0.layers.7._codebook.embed_avg\", \"rq.rvqs.0.layers.7._codebook.embed\", \"decoder_film.to_cond.weight\", \"decoder_film.to_cond.bias\", \"decoder_attn.layers.0.0.q_scale\", \"decoder_attn.layers.0.0.k_scale\", \"decoder_attn.layers.0.0.norm.weight\", \"decoder_attn.layers.0.0.norm.bias\", \"decoder_attn.layers.0.0.to_qkv.weight\", \"decoder_attn.layers.0.0.attn_fn.rel_pos.inv_freq\", \"decoder_attn.layers.0.0.to_out.weight\", \"decoder_attn.layers.0.1.0.weight\", \"decoder_attn.layers.0.1.0.bias\", \"decoder_attn.layers.0.1.1.weight\", \"decoder_attn.layers.0.1.4.weight\", \"decoder.0.conv.weight\", \"decoder.0.conv.bias\", \"decoder.1.0.conv.weight\", \"decoder.1.0.conv.bias\", \"decoder.1.1.fn.0.conv.weight\", \"decoder.1.1.fn.0.conv.bias\", \"decoder.1.1.fn.2.conv.weight\", \"decoder.1.1.fn.2.conv.bias\", \"decoder.1.2.fn.0.conv.weight\", \"decoder.1.2.fn.0.conv.bias\", \"decoder.1.2.fn.2.conv.weight\", \"decoder.1.2.fn.2.conv.bias\", \"decoder.1.3.fn.0.conv.weight\", \"decoder.1.3.fn.0.conv.bias\", \"decoder.1.3.fn.2.conv.weight\", \"decoder.1.3.fn.2.conv.bias\", \"decoder.2.0.conv.weight\", \"decoder.2.0.conv.bias\", \"decoder.2.1.fn.0.conv.weight\", \"decoder.2.1.fn.0.conv.bias\", \"decoder.2.1.fn.2.conv.weight\", \"decoder.2.1.fn.2.conv.bias\", \"decoder.2.2.fn.0.conv.weight\", \"decoder.2.2.fn.0.conv.bias\", \"decoder.2.2.fn.2.conv.weight\", \"decoder.2.2.fn.2.conv.bias\", \"decoder.2.3.fn.0.conv.weight\", \"decoder.2.3.fn.0.conv.bias\", \"decoder.2.3.fn.2.conv.weight\", \"decoder.2.3.fn.2.conv.bias\", \"decoder.3.0.conv.weight\", \"decoder.3.0.conv.bias\", \"decoder.3.1.fn.0.conv.weight\", \"decoder.3.1.fn.0.conv.bias\", \"decoder.3.1.fn.2.conv.weight\", \"decoder.3.1.fn.2.conv.bias\", \"decoder.3.2.fn.0.conv.weight\", \"decoder.3.2.fn.0.conv.bias\", \"decoder.3.2.fn.2.conv.weight\", \"decoder.3.2.fn.2.conv.bias\", \"decoder.3.3.fn.0.conv.weight\", \"decoder.3.3.fn.0.conv.bias\", \"decoder.3.3.fn.2.conv.weight\", \"decoder.3.3.fn.2.conv.bias\", \"decoder.4.0.conv.weight\", \"decoder.4.0.conv.bias\", \"decoder.4.1.fn.0.conv.weight\", \"decoder.4.1.fn.0.conv.bias\", \"decoder.4.1.fn.2.conv.weight\", \"decoder.4.1.fn.2.conv.bias\", \"decoder.4.2.fn.0.conv.weight\", \"decoder.4.2.fn.0.conv.bias\", \"decoder.4.2.fn.2.conv.weight\", \"decoder.4.2.fn.2.conv.bias\", \"decoder.4.3.fn.0.conv.weight\", \"decoder.4.3.fn.0.conv.bias\", \"decoder.4.3.fn.2.conv.weight\", \"decoder.4.3.fn.2.conv.bias\", \"decoder.5.conv.weight\", \"decoder.5.conv.bias\", \"discriminators.0.init_conv.weight\", \"discriminators.0.init_conv.bias\", \"discriminators.0.conv_layers.0.0.weight\", \"discriminators.0.conv_layers.0.0.bias\", \"discriminators.0.conv_layers.1.0.weight\", \"discriminators.0.conv_layers.1.0.bias\", \"discriminators.0.conv_layers.2.0.weight\", \"discriminators.0.conv_layers.2.0.bias\", \"discriminators.0.conv_layers.3.0.weight\", \"discriminators.0.conv_layers.3.0.bias\", \"discriminators.0.final_conv.0.weight\", \"discriminators.0.final_conv.0.bias\", \"discriminators.0.final_conv.2.weight\", \"discriminators.0.final_conv.2.bias\", \"discriminators.1.init_conv.weight\", \"discriminators.1.init_conv.bias\", \"discriminators.1.conv_layers.0.0.weight\", \"discriminators.1.conv_layers.0.0.bias\", \"discriminators.1.conv_layers.1.0.weight\", \"discriminators.1.conv_layers.1.0.bias\", \"discriminators.1.conv_layers.2.0.weight\", \"discriminators.1.conv_layers.2.0.bias\", \"discriminators.1.conv_layers.3.0.weight\", \"discriminators.1.conv_layers.3.0.bias\", \"discriminators.1.final_conv.0.weight\", \"discriminators.1.final_conv.0.bias\", \"discriminators.1.final_conv.2.weight\", \"discriminators.1.final_conv.2.bias\", \"discriminators.2.init_conv.weight\", \"discriminators.2.init_conv.bias\", \"discriminators.2.conv_layers.0.0.weight\", \"discriminators.2.conv_layers.0.0.bias\", \"discriminators.2.conv_layers.1.0.weight\", \"discriminators.2.conv_layers.1.0.bias\", \"discriminators.2.conv_layers.2.0.weight\", \"discriminators.2.conv_layers.2.0.bias\", \"discriminators.2.conv_layers.3.0.weight\", \"discriminators.2.conv_layers.3.0.bias\", \"discriminators.2.final_conv.0.weight\", \"discriminators.2.final_conv.0.bias\", \"discriminators.2.final_conv.2.weight\", \"discriminators.2.final_conv.2.bias\", \"stft_discriminator.init_conv.weight\", \"stft_discriminator.init_conv.bias\", \"stft_discriminator.layers.0.0.fn.0.weight\", \"stft_discriminator.layers.0.0.fn.0.bias\", \"stft_discriminator.layers.0.0.fn.1.b\", \"stft_discriminator.layers.0.0.fn.2.weight\", \"stft_discriminator.layers.0.0.fn.2.bias\", \"stft_discriminator.layers.0.1.weight\", \"stft_discriminator.layers.0.1.bias\", \"stft_discriminator.layers.1.0.fn.0.weight\", \"stft_discriminator.layers.1.0.fn.0.bias\", \"stft_discriminator.layers.1.0.fn.1.b\", \"stft_discriminator.layers.1.0.fn.2.weight\", \"stft_discriminator.layers.1.0.fn.2.bias\", \"stft_discriminator.layers.1.1.weight\", \"stft_discriminator.layers.1.1.bias\", \"stft_discriminator.layers.2.0.fn.0.weight\", \"stft_discriminator.layers.2.0.fn.0.bias\", \"stft_discriminator.layers.2.0.fn.1.b\", \"stft_discriminator.layers.2.0.fn.2.weight\", \"stft_discriminator.layers.2.0.fn.2.bias\", \"stft_discriminator.layers.2.1.weight\", \"stft_discriminator.layers.2.1.bias\", \"stft_discriminator.layers.3.0.fn.0.weight\", \"stft_discriminator.layers.3.0.fn.0.bias\", \"stft_discriminator.layers.3.0.fn.1.b\", \"stft_discriminator.layers.3.0.fn.2.weight\", \"stft_discriminator.layers.3.0.fn.2.bias\", \"stft_discriminator.layers.3.1.weight\", \"stft_discriminator.layers.3.1.bias\", \"stft_discriminator.layers.4.0.fn.0.weight\", \"stft_discriminator.layers.4.0.fn.0.bias\", \"stft_discriminator.layers.4.0.fn.1.b\", \"stft_discriminator.layers.4.0.fn.2.weight\", \"stft_discriminator.layers.4.0.fn.2.bias\", \"stft_discriminator.layers.4.1.weight\", \"stft_discriminator.layers.4.1.bias\", \"stft_discriminator.layers.5.0.fn.0.weight\", \"stft_discriminator.layers.5.0.fn.0.bias\", \"stft_discriminator.layers.5.0.fn.1.b\", \"stft_discriminator.layers.5.0.fn.2.weight\", \"stft_discriminator.layers.5.0.fn.2.bias\", \"stft_discriminator.layers.5.1.weight\", \"stft_discriminator.layers.5.1.bias\", \"stft_discriminator.final_conv.weight\", \"stft_discriminator.final_conv.bias\", \"mel_spec_transforms.0.spectrogram.window\", \"mel_spec_transforms.0.mel_scale.fb\", \"mel_spec_transforms.1.spectrogram.window\", \"mel_spec_transforms.1.mel_scale.fb\", \"mel_spec_transforms.2.spectrogram.window\", \"mel_spec_transforms.2.mel_scale.fb\", \"mel_spec_transforms.3.spectrogram.window\", \"mel_spec_transforms.3.mel_scale.fb\", \"mel_spec_transforms.4.spectrogram.window\", \"mel_spec_transforms.4.mel_scale.fb\", \"mel_spec_transforms.5.spectrogram.window\", \"mel_spec_transforms.5.mel_scale.fb\". \n\tUnexpected key(s) in state_dict: \"start_token\", \"semantic_embedding.weight\", \"proj_text_embed.weight\", \"transformer.layers.0.0.norm.gamma\", \"transformer.layers.0.0.norm.beta\", \"transformer.layers.0.0.to_q.weight\", \"transformer.layers.0.0.to_kv.weight\", \"transformer.layers.0.0.to_out.0.weight\", \"transformer.layers.0.2.0.gamma\", \"transformer.layers.0.2.0.beta\", \"transformer.layers.0.2.1.weight\", \"transformer.layers.0.2.3.gamma\", \"transformer.layers.0.2.3.beta\", \"transformer.layers.0.2.5.weight\", \"transformer.layers.1.0.norm.gamma\", \"transformer.layers.1.0.norm.beta\", \"transformer.layers.1.0.to_q.weight\", \"transformer.layers.1.0.to_kv.weight\", \"transformer.layers.1.0.to_out.0.weight\", \"transformer.layers.1.2.0.gamma\", \"transformer.layers.1.2.0.beta\", \"transformer.layers.1.2.1.weight\", \"transformer.layers.1.2.3.gamma\", \"transformer.layers.1.2.3.beta\", \"transformer.layers.1.2.5.weight\", \"transformer.layers.2.0.norm.gamma\", \"transformer.layers.2.0.norm.beta\", \"transformer.layers.2.0.to_q.weight\", \"transformer.layers.2.0.to_kv.weight\", \"transformer.layers.2.0.to_out.0.weight\", \"transformer.layers.2.2.0.gamma\", \"transformer.layers.2.2.0.beta\", \"transformer.layers.2.2.1.weight\", \"transformer.layers.2.2.3.gamma\", \"transformer.layers.2.2.3.beta\", \"transformer.layers.2.2.5.weight\", \"transformer.layers.3.0.norm.gamma\", \"transformer.layers.3.0.norm.beta\", \"transformer.layers.3.0.to_q.weight\", \"transformer.layers.3.0.to_kv.weight\", \"transformer.layers.3.0.to_out.0.weight\", \"transformer.layers.3.2.0.gamma\", \"transformer.layers.3.2.0.beta\", \"transformer.layers.3.2.1.weight\", \"transformer.layers.3.2.3.gamma\", \"transformer.layers.3.2.3.beta\", \"transformer.layers.3.2.5.weight\", \"transformer.layers.4.0.norm.gamma\", \"transformer.layers.4.0.norm.beta\", \"transformer.layers.4.0.to_q.weight\", \"transformer.layers.4.0.to_kv.weight\", \"transformer.layers.4.0.to_out.0.weight\", \"transformer.layers.4.2.0.gamma\", \"transformer.layers.4.2.0.beta\", \"transformer.layers.4.2.1.weight\", \"transformer.layers.4.2.3.gamma\", \"transformer.layers.4.2.3.beta\", \"transformer.layers.4.2.5.weight\", \"transformer.layers.5.0.norm.gamma\", \"transformer.layers.5.0.norm.beta\", \"transformer.layers.5.0.to_q.weight\", \"transformer.layers.5.0.to_kv.weight\", \"transformer.layers.5.0.to_out.0.weight\", \"transformer.layers.5.2.0.gamma\", \"transformer.layers.5.2.0.beta\", \"transformer.layers.5.2.1.weight\", \"transformer.layers.5.2.3.gamma\", \"transformer.layers.5.2.3.beta\", \"transformer.layers.5.2.5.weight\", \"transformer.rel_pos_bias.net.0.0.weight\", \"transformer.rel_pos_bias.net.0.0.bias\", \"transformer.rel_pos_bias.net.1.0.weight\", \"transformer.rel_pos_bias.net.1.0.bias\", \"transformer.rel_pos_bias.net.2.0.weight\", \"transformer.rel_pos_bias.net.2.0.bias\", \"transformer.rel_pos_bias.net.3.weight\", \"transformer.rel_pos_bias.net.3.bias\", \"transformer.norm.gamma\", \"transformer.norm.beta\", \"to_logits.weight\", \"to_logits.bias\". "
     ]
    }
   ],
   "source": [
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "# semantic.transformer.0.pt\n",
    "soundstream_ckpt = Path(\"./results/semantic.transformer.0.pt\")\n",
    "assert soundstream_ckpt.exists()\n",
    "soundstream.load(str(soundstream_ckpt))\n",
    "\n",
    "coarse_transformer = CoarseTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    codebook_size = 1024,\n",
    "    num_coarse_quantizers = 3,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = CoarseTransformerTrainer(\n",
    "    transformer = coarse_transformer,\n",
    "    codec = soundstream,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ab6d0b1-b4a4-42a2-8943-5c5bdc644f25",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SoundStream:\n\tMissing key(s) in state_dict: \"encoder.0.conv.weight\", \"encoder.0.conv.bias\", \"encoder.1.0.fn.0.conv.weight\", \"encoder.1.0.fn.0.conv.bias\", \"encoder.1.0.fn.2.conv.weight\", \"encoder.1.0.fn.2.conv.bias\", \"encoder.1.1.fn.0.conv.weight\", \"encoder.1.1.fn.0.conv.bias\", \"encoder.1.1.fn.2.conv.weight\", \"encoder.1.1.fn.2.conv.bias\", \"encoder.1.2.fn.0.conv.weight\", \"encoder.1.2.fn.0.conv.bias\", \"encoder.1.2.fn.2.conv.weight\", \"encoder.1.2.fn.2.conv.bias\", \"encoder.1.3.conv.weight\", \"encoder.1.3.conv.bias\", \"encoder.2.0.fn.0.conv.weight\", \"encoder.2.0.fn.0.conv.bias\", \"encoder.2.0.fn.2.conv.weight\", \"encoder.2.0.fn.2.conv.bias\", \"encoder.2.1.fn.0.conv.weight\", \"encoder.2.1.fn.0.conv.bias\", \"encoder.2.1.fn.2.conv.weight\", \"encoder.2.1.fn.2.conv.bias\", \"encoder.2.2.fn.0.conv.weight\", \"encoder.2.2.fn.0.conv.bias\", \"encoder.2.2.fn.2.conv.weight\", \"encoder.2.2.fn.2.conv.bias\", \"encoder.2.3.conv.weight\", \"encoder.2.3.conv.bias\", \"encoder.3.0.fn.0.conv.weight\", \"encoder.3.0.fn.0.conv.bias\", \"encoder.3.0.fn.2.conv.weight\", \"encoder.3.0.fn.2.conv.bias\", \"encoder.3.1.fn.0.conv.weight\", \"encoder.3.1.fn.0.conv.bias\", \"encoder.3.1.fn.2.conv.weight\", \"encoder.3.1.fn.2.conv.bias\", \"encoder.3.2.fn.0.conv.weight\", \"encoder.3.2.fn.0.conv.bias\", \"encoder.3.2.fn.2.conv.weight\", \"encoder.3.2.fn.2.conv.bias\", \"encoder.3.3.conv.weight\", \"encoder.3.3.conv.bias\", \"encoder.4.0.fn.0.conv.weight\", \"encoder.4.0.fn.0.conv.bias\", \"encoder.4.0.fn.2.conv.weight\", \"encoder.4.0.fn.2.conv.bias\", \"encoder.4.1.fn.0.conv.weight\", \"encoder.4.1.fn.0.conv.bias\", \"encoder.4.1.fn.2.conv.weight\", \"encoder.4.1.fn.2.conv.bias\", \"encoder.4.2.fn.0.conv.weight\", \"encoder.4.2.fn.0.conv.bias\", \"encoder.4.2.fn.2.conv.weight\", \"encoder.4.2.fn.2.conv.bias\", \"encoder.4.3.conv.weight\", \"encoder.4.3.conv.bias\", \"encoder.5.conv.weight\", \"encoder.5.conv.bias\", \"encoder_attn.layers.0.0.q_scale\", \"encoder_attn.layers.0.0.k_scale\", \"encoder_attn.layers.0.0.norm.weight\", \"encoder_attn.layers.0.0.norm.bias\", \"encoder_attn.layers.0.0.to_qkv.weight\", \"encoder_attn.layers.0.0.attn_fn.rel_pos.inv_freq\", \"encoder_attn.layers.0.0.to_out.weight\", \"encoder_attn.layers.0.1.0.weight\", \"encoder_attn.layers.0.1.0.bias\", \"encoder_attn.layers.0.1.1.weight\", \"encoder_attn.layers.0.1.4.weight\", \"encoder_film.to_cond.weight\", \"encoder_film.to_cond.bias\", \"rq.rvqs.0.layers.0._codebook.initted\", \"rq.rvqs.0.layers.0._codebook.cluster_size\", \"rq.rvqs.0.layers.0._codebook.embed_avg\", \"rq.rvqs.0.layers.0._codebook.embed\", \"rq.rvqs.0.layers.1._codebook.initted\", \"rq.rvqs.0.layers.1._codebook.cluster_size\", \"rq.rvqs.0.layers.1._codebook.embed_avg\", \"rq.rvqs.0.layers.1._codebook.embed\", \"rq.rvqs.0.layers.2._codebook.initted\", \"rq.rvqs.0.layers.2._codebook.cluster_size\", \"rq.rvqs.0.layers.2._codebook.embed_avg\", \"rq.rvqs.0.layers.2._codebook.embed\", \"rq.rvqs.0.layers.3._codebook.initted\", \"rq.rvqs.0.layers.3._codebook.cluster_size\", \"rq.rvqs.0.layers.3._codebook.embed_avg\", \"rq.rvqs.0.layers.3._codebook.embed\", \"rq.rvqs.0.layers.4._codebook.initted\", \"rq.rvqs.0.layers.4._codebook.cluster_size\", \"rq.rvqs.0.layers.4._codebook.embed_avg\", \"rq.rvqs.0.layers.4._codebook.embed\", \"rq.rvqs.0.layers.5._codebook.initted\", \"rq.rvqs.0.layers.5._codebook.cluster_size\", \"rq.rvqs.0.layers.5._codebook.embed_avg\", \"rq.rvqs.0.layers.5._codebook.embed\", \"rq.rvqs.0.layers.6._codebook.initted\", \"rq.rvqs.0.layers.6._codebook.cluster_size\", \"rq.rvqs.0.layers.6._codebook.embed_avg\", \"rq.rvqs.0.layers.6._codebook.embed\", \"rq.rvqs.0.layers.7._codebook.initted\", \"rq.rvqs.0.layers.7._codebook.cluster_size\", \"rq.rvqs.0.layers.7._codebook.embed_avg\", \"rq.rvqs.0.layers.7._codebook.embed\", \"decoder_film.to_cond.weight\", \"decoder_film.to_cond.bias\", \"decoder_attn.layers.0.0.q_scale\", \"decoder_attn.layers.0.0.k_scale\", \"decoder_attn.layers.0.0.norm.weight\", \"decoder_attn.layers.0.0.norm.bias\", \"decoder_attn.layers.0.0.to_qkv.weight\", \"decoder_attn.layers.0.0.attn_fn.rel_pos.inv_freq\", \"decoder_attn.layers.0.0.to_out.weight\", \"decoder_attn.layers.0.1.0.weight\", \"decoder_attn.layers.0.1.0.bias\", \"decoder_attn.layers.0.1.1.weight\", \"decoder_attn.layers.0.1.4.weight\", \"decoder.0.conv.weight\", \"decoder.0.conv.bias\", \"decoder.1.0.conv.weight\", \"decoder.1.0.conv.bias\", \"decoder.1.1.fn.0.conv.weight\", \"decoder.1.1.fn.0.conv.bias\", \"decoder.1.1.fn.2.conv.weight\", \"decoder.1.1.fn.2.conv.bias\", \"decoder.1.2.fn.0.conv.weight\", \"decoder.1.2.fn.0.conv.bias\", \"decoder.1.2.fn.2.conv.weight\", \"decoder.1.2.fn.2.conv.bias\", \"decoder.1.3.fn.0.conv.weight\", \"decoder.1.3.fn.0.conv.bias\", \"decoder.1.3.fn.2.conv.weight\", \"decoder.1.3.fn.2.conv.bias\", \"decoder.2.0.conv.weight\", \"decoder.2.0.conv.bias\", \"decoder.2.1.fn.0.conv.weight\", \"decoder.2.1.fn.0.conv.bias\", \"decoder.2.1.fn.2.conv.weight\", \"decoder.2.1.fn.2.conv.bias\", \"decoder.2.2.fn.0.conv.weight\", \"decoder.2.2.fn.0.conv.bias\", \"decoder.2.2.fn.2.conv.weight\", \"decoder.2.2.fn.2.conv.bias\", \"decoder.2.3.fn.0.conv.weight\", \"decoder.2.3.fn.0.conv.bias\", \"decoder.2.3.fn.2.conv.weight\", \"decoder.2.3.fn.2.conv.bias\", \"decoder.3.0.conv.weight\", \"decoder.3.0.conv.bias\", \"decoder.3.1.fn.0.conv.weight\", \"decoder.3.1.fn.0.conv.bias\", \"decoder.3.1.fn.2.conv.weight\", \"decoder.3.1.fn.2.conv.bias\", \"decoder.3.2.fn.0.conv.weight\", \"decoder.3.2.fn.0.conv.bias\", \"decoder.3.2.fn.2.conv.weight\", \"decoder.3.2.fn.2.conv.bias\", \"decoder.3.3.fn.0.conv.weight\", \"decoder.3.3.fn.0.conv.bias\", \"decoder.3.3.fn.2.conv.weight\", \"decoder.3.3.fn.2.conv.bias\", \"decoder.4.0.conv.weight\", \"decoder.4.0.conv.bias\", \"decoder.4.1.fn.0.conv.weight\", \"decoder.4.1.fn.0.conv.bias\", \"decoder.4.1.fn.2.conv.weight\", \"decoder.4.1.fn.2.conv.bias\", \"decoder.4.2.fn.0.conv.weight\", \"decoder.4.2.fn.0.conv.bias\", \"decoder.4.2.fn.2.conv.weight\", \"decoder.4.2.fn.2.conv.bias\", \"decoder.4.3.fn.0.conv.weight\", \"decoder.4.3.fn.0.conv.bias\", \"decoder.4.3.fn.2.conv.weight\", \"decoder.4.3.fn.2.conv.bias\", \"decoder.5.conv.weight\", \"decoder.5.conv.bias\", \"discriminators.0.init_conv.weight\", \"discriminators.0.init_conv.bias\", \"discriminators.0.conv_layers.0.0.weight\", \"discriminators.0.conv_layers.0.0.bias\", \"discriminators.0.conv_layers.1.0.weight\", \"discriminators.0.conv_layers.1.0.bias\", \"discriminators.0.conv_layers.2.0.weight\", \"discriminators.0.conv_layers.2.0.bias\", \"discriminators.0.conv_layers.3.0.weight\", \"discriminators.0.conv_layers.3.0.bias\", \"discriminators.0.final_conv.0.weight\", \"discriminators.0.final_conv.0.bias\", \"discriminators.0.final_conv.2.weight\", \"discriminators.0.final_conv.2.bias\", \"discriminators.1.init_conv.weight\", \"discriminators.1.init_conv.bias\", \"discriminators.1.conv_layers.0.0.weight\", \"discriminators.1.conv_layers.0.0.bias\", \"discriminators.1.conv_layers.1.0.weight\", \"discriminators.1.conv_layers.1.0.bias\", \"discriminators.1.conv_layers.2.0.weight\", \"discriminators.1.conv_layers.2.0.bias\", \"discriminators.1.conv_layers.3.0.weight\", \"discriminators.1.conv_layers.3.0.bias\", \"discriminators.1.final_conv.0.weight\", \"discriminators.1.final_conv.0.bias\", \"discriminators.1.final_conv.2.weight\", \"discriminators.1.final_conv.2.bias\", \"discriminators.2.init_conv.weight\", \"discriminators.2.init_conv.bias\", \"discriminators.2.conv_layers.0.0.weight\", \"discriminators.2.conv_layers.0.0.bias\", \"discriminators.2.conv_layers.1.0.weight\", \"discriminators.2.conv_layers.1.0.bias\", \"discriminators.2.conv_layers.2.0.weight\", \"discriminators.2.conv_layers.2.0.bias\", \"discriminators.2.conv_layers.3.0.weight\", \"discriminators.2.conv_layers.3.0.bias\", \"discriminators.2.final_conv.0.weight\", \"discriminators.2.final_conv.0.bias\", \"discriminators.2.final_conv.2.weight\", \"discriminators.2.final_conv.2.bias\", \"stft_discriminator.init_conv.weight\", \"stft_discriminator.init_conv.bias\", \"stft_discriminator.layers.0.0.fn.0.weight\", \"stft_discriminator.layers.0.0.fn.0.bias\", \"stft_discriminator.layers.0.0.fn.1.b\", \"stft_discriminator.layers.0.0.fn.2.weight\", \"stft_discriminator.layers.0.0.fn.2.bias\", \"stft_discriminator.layers.0.1.weight\", \"stft_discriminator.layers.0.1.bias\", \"stft_discriminator.layers.1.0.fn.0.weight\", \"stft_discriminator.layers.1.0.fn.0.bias\", \"stft_discriminator.layers.1.0.fn.1.b\", \"stft_discriminator.layers.1.0.fn.2.weight\", \"stft_discriminator.layers.1.0.fn.2.bias\", \"stft_discriminator.layers.1.1.weight\", \"stft_discriminator.layers.1.1.bias\", \"stft_discriminator.layers.2.0.fn.0.weight\", \"stft_discriminator.layers.2.0.fn.0.bias\", \"stft_discriminator.layers.2.0.fn.1.b\", \"stft_discriminator.layers.2.0.fn.2.weight\", \"stft_discriminator.layers.2.0.fn.2.bias\", \"stft_discriminator.layers.2.1.weight\", \"stft_discriminator.layers.2.1.bias\", \"stft_discriminator.layers.3.0.fn.0.weight\", \"stft_discriminator.layers.3.0.fn.0.bias\", \"stft_discriminator.layers.3.0.fn.1.b\", \"stft_discriminator.layers.3.0.fn.2.weight\", \"stft_discriminator.layers.3.0.fn.2.bias\", \"stft_discriminator.layers.3.1.weight\", \"stft_discriminator.layers.3.1.bias\", \"stft_discriminator.layers.4.0.fn.0.weight\", \"stft_discriminator.layers.4.0.fn.0.bias\", \"stft_discriminator.layers.4.0.fn.1.b\", \"stft_discriminator.layers.4.0.fn.2.weight\", \"stft_discriminator.layers.4.0.fn.2.bias\", \"stft_discriminator.layers.4.1.weight\", \"stft_discriminator.layers.4.1.bias\", \"stft_discriminator.layers.5.0.fn.0.weight\", \"stft_discriminator.layers.5.0.fn.0.bias\", \"stft_discriminator.layers.5.0.fn.1.b\", \"stft_discriminator.layers.5.0.fn.2.weight\", \"stft_discriminator.layers.5.0.fn.2.bias\", \"stft_discriminator.layers.5.1.weight\", \"stft_discriminator.layers.5.1.bias\", \"stft_discriminator.final_conv.weight\", \"stft_discriminator.final_conv.bias\", \"mel_spec_transforms.0.spectrogram.window\", \"mel_spec_transforms.0.mel_scale.fb\", \"mel_spec_transforms.1.spectrogram.window\", \"mel_spec_transforms.1.mel_scale.fb\", \"mel_spec_transforms.2.spectrogram.window\", \"mel_spec_transforms.2.mel_scale.fb\", \"mel_spec_transforms.3.spectrogram.window\", \"mel_spec_transforms.3.mel_scale.fb\", \"mel_spec_transforms.4.spectrogram.window\", \"mel_spec_transforms.4.mel_scale.fb\", \"mel_spec_transforms.5.spectrogram.window\", \"mel_spec_transforms.5.mel_scale.fb\". \n\tUnexpected key(s) in state_dict: \"start_token\", \"semantic_embedding.weight\", \"proj_text_embed.weight\", \"transformer.layers.0.0.norm.gamma\", \"transformer.layers.0.0.norm.beta\", \"transformer.layers.0.0.to_q.weight\", \"transformer.layers.0.0.to_kv.weight\", \"transformer.layers.0.0.to_out.0.weight\", \"transformer.layers.0.2.0.gamma\", \"transformer.layers.0.2.0.beta\", \"transformer.layers.0.2.1.weight\", \"transformer.layers.0.2.3.gamma\", \"transformer.layers.0.2.3.beta\", \"transformer.layers.0.2.5.weight\", \"transformer.layers.1.0.norm.gamma\", \"transformer.layers.1.0.norm.beta\", \"transformer.layers.1.0.to_q.weight\", \"transformer.layers.1.0.to_kv.weight\", \"transformer.layers.1.0.to_out.0.weight\", \"transformer.layers.1.2.0.gamma\", \"transformer.layers.1.2.0.beta\", \"transformer.layers.1.2.1.weight\", \"transformer.layers.1.2.3.gamma\", \"transformer.layers.1.2.3.beta\", \"transformer.layers.1.2.5.weight\", \"transformer.layers.2.0.norm.gamma\", \"transformer.layers.2.0.norm.beta\", \"transformer.layers.2.0.to_q.weight\", \"transformer.layers.2.0.to_kv.weight\", \"transformer.layers.2.0.to_out.0.weight\", \"transformer.layers.2.2.0.gamma\", \"transformer.layers.2.2.0.beta\", \"transformer.layers.2.2.1.weight\", \"transformer.layers.2.2.3.gamma\", \"transformer.layers.2.2.3.beta\", \"transformer.layers.2.2.5.weight\", \"transformer.layers.3.0.norm.gamma\", \"transformer.layers.3.0.norm.beta\", \"transformer.layers.3.0.to_q.weight\", \"transformer.layers.3.0.to_kv.weight\", \"transformer.layers.3.0.to_out.0.weight\", \"transformer.layers.3.2.0.gamma\", \"transformer.layers.3.2.0.beta\", \"transformer.layers.3.2.1.weight\", \"transformer.layers.3.2.3.gamma\", \"transformer.layers.3.2.3.beta\", \"transformer.layers.3.2.5.weight\", \"transformer.layers.4.0.norm.gamma\", \"transformer.layers.4.0.norm.beta\", \"transformer.layers.4.0.to_q.weight\", \"transformer.layers.4.0.to_kv.weight\", \"transformer.layers.4.0.to_out.0.weight\", \"transformer.layers.4.2.0.gamma\", \"transformer.layers.4.2.0.beta\", \"transformer.layers.4.2.1.weight\", \"transformer.layers.4.2.3.gamma\", \"transformer.layers.4.2.3.beta\", \"transformer.layers.4.2.5.weight\", \"transformer.layers.5.0.norm.gamma\", \"transformer.layers.5.0.norm.beta\", \"transformer.layers.5.0.to_q.weight\", \"transformer.layers.5.0.to_kv.weight\", \"transformer.layers.5.0.to_out.0.weight\", \"transformer.layers.5.2.0.gamma\", \"transformer.layers.5.2.0.beta\", \"transformer.layers.5.2.1.weight\", \"transformer.layers.5.2.3.gamma\", \"transformer.layers.5.2.3.beta\", \"transformer.layers.5.2.5.weight\", \"transformer.rel_pos_bias.net.0.0.weight\", \"transformer.rel_pos_bias.net.0.0.bias\", \"transformer.rel_pos_bias.net.1.0.weight\", \"transformer.rel_pos_bias.net.1.0.bias\", \"transformer.rel_pos_bias.net.2.0.weight\", \"transformer.rel_pos_bias.net.2.0.bias\", \"transformer.rel_pos_bias.net.3.weight\", \"transformer.rel_pos_bias.net.3.bias\", \"transformer.norm.gamma\", \"transformer.norm.beta\", \"to_logits.weight\", \"to_logits.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m soundstream \u001b[38;5;241m=\u001b[39m SoundStream(\n\u001b[1;32m      2\u001b[0m     codebook_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      3\u001b[0m     rq_num_quantizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[43msoundstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msoundstream_ckpt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m fine_transformer \u001b[38;5;241m=\u001b[39m FineTransformer(\n\u001b[1;32m      9\u001b[0m     num_coarse_quantizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     10\u001b[0m     num_fine_quantizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m FineTransformerTrainer(\n\u001b[1;32m     17\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m fine_transformer,\n\u001b[1;32m     18\u001b[0m     codec \u001b[38;5;241m=\u001b[39m soundstream,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     num_train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/py/audiolm-pytorch/audiolm_pytorch/soundstream.py:664\u001b[0m, in \u001b[0;36mSoundStream.load\u001b[0;34m(self, path, strict)\u001b[0m\n\u001b[1;32m    661\u001b[0m     model_pkg \u001b[38;5;241m=\u001b[39m filter_by_keys(\u001b[38;5;28;01mlambda\u001b[39;00m k: k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema_model.\u001b[39m\u001b[38;5;124m'\u001b[39m), model_pkg)\n\u001b[1;32m    662\u001b[0m     model_pkg \u001b[38;5;241m=\u001b[39m map_keys(\u001b[38;5;28;01mlambda\u001b[39;00m k: k[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema_model.\u001b[39m\u001b[38;5;124m'\u001b[39m):], model_pkg)\n\u001b[0;32m--> 664\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audiolm/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SoundStream:\n\tMissing key(s) in state_dict: \"encoder.0.conv.weight\", \"encoder.0.conv.bias\", \"encoder.1.0.fn.0.conv.weight\", \"encoder.1.0.fn.0.conv.bias\", \"encoder.1.0.fn.2.conv.weight\", \"encoder.1.0.fn.2.conv.bias\", \"encoder.1.1.fn.0.conv.weight\", \"encoder.1.1.fn.0.conv.bias\", \"encoder.1.1.fn.2.conv.weight\", \"encoder.1.1.fn.2.conv.bias\", \"encoder.1.2.fn.0.conv.weight\", \"encoder.1.2.fn.0.conv.bias\", \"encoder.1.2.fn.2.conv.weight\", \"encoder.1.2.fn.2.conv.bias\", \"encoder.1.3.conv.weight\", \"encoder.1.3.conv.bias\", \"encoder.2.0.fn.0.conv.weight\", \"encoder.2.0.fn.0.conv.bias\", \"encoder.2.0.fn.2.conv.weight\", \"encoder.2.0.fn.2.conv.bias\", \"encoder.2.1.fn.0.conv.weight\", \"encoder.2.1.fn.0.conv.bias\", \"encoder.2.1.fn.2.conv.weight\", \"encoder.2.1.fn.2.conv.bias\", \"encoder.2.2.fn.0.conv.weight\", \"encoder.2.2.fn.0.conv.bias\", \"encoder.2.2.fn.2.conv.weight\", \"encoder.2.2.fn.2.conv.bias\", \"encoder.2.3.conv.weight\", \"encoder.2.3.conv.bias\", \"encoder.3.0.fn.0.conv.weight\", \"encoder.3.0.fn.0.conv.bias\", \"encoder.3.0.fn.2.conv.weight\", \"encoder.3.0.fn.2.conv.bias\", \"encoder.3.1.fn.0.conv.weight\", \"encoder.3.1.fn.0.conv.bias\", \"encoder.3.1.fn.2.conv.weight\", \"encoder.3.1.fn.2.conv.bias\", \"encoder.3.2.fn.0.conv.weight\", \"encoder.3.2.fn.0.conv.bias\", \"encoder.3.2.fn.2.conv.weight\", \"encoder.3.2.fn.2.conv.bias\", \"encoder.3.3.conv.weight\", \"encoder.3.3.conv.bias\", \"encoder.4.0.fn.0.conv.weight\", \"encoder.4.0.fn.0.conv.bias\", \"encoder.4.0.fn.2.conv.weight\", \"encoder.4.0.fn.2.conv.bias\", \"encoder.4.1.fn.0.conv.weight\", \"encoder.4.1.fn.0.conv.bias\", \"encoder.4.1.fn.2.conv.weight\", \"encoder.4.1.fn.2.conv.bias\", \"encoder.4.2.fn.0.conv.weight\", \"encoder.4.2.fn.0.conv.bias\", \"encoder.4.2.fn.2.conv.weight\", \"encoder.4.2.fn.2.conv.bias\", \"encoder.4.3.conv.weight\", \"encoder.4.3.conv.bias\", \"encoder.5.conv.weight\", \"encoder.5.conv.bias\", \"encoder_attn.layers.0.0.q_scale\", \"encoder_attn.layers.0.0.k_scale\", \"encoder_attn.layers.0.0.norm.weight\", \"encoder_attn.layers.0.0.norm.bias\", \"encoder_attn.layers.0.0.to_qkv.weight\", \"encoder_attn.layers.0.0.attn_fn.rel_pos.inv_freq\", \"encoder_attn.layers.0.0.to_out.weight\", \"encoder_attn.layers.0.1.0.weight\", \"encoder_attn.layers.0.1.0.bias\", \"encoder_attn.layers.0.1.1.weight\", \"encoder_attn.layers.0.1.4.weight\", \"encoder_film.to_cond.weight\", \"encoder_film.to_cond.bias\", \"rq.rvqs.0.layers.0._codebook.initted\", \"rq.rvqs.0.layers.0._codebook.cluster_size\", \"rq.rvqs.0.layers.0._codebook.embed_avg\", \"rq.rvqs.0.layers.0._codebook.embed\", \"rq.rvqs.0.layers.1._codebook.initted\", \"rq.rvqs.0.layers.1._codebook.cluster_size\", \"rq.rvqs.0.layers.1._codebook.embed_avg\", \"rq.rvqs.0.layers.1._codebook.embed\", \"rq.rvqs.0.layers.2._codebook.initted\", \"rq.rvqs.0.layers.2._codebook.cluster_size\", \"rq.rvqs.0.layers.2._codebook.embed_avg\", \"rq.rvqs.0.layers.2._codebook.embed\", \"rq.rvqs.0.layers.3._codebook.initted\", \"rq.rvqs.0.layers.3._codebook.cluster_size\", \"rq.rvqs.0.layers.3._codebook.embed_avg\", \"rq.rvqs.0.layers.3._codebook.embed\", \"rq.rvqs.0.layers.4._codebook.initted\", \"rq.rvqs.0.layers.4._codebook.cluster_size\", \"rq.rvqs.0.layers.4._codebook.embed_avg\", \"rq.rvqs.0.layers.4._codebook.embed\", \"rq.rvqs.0.layers.5._codebook.initted\", \"rq.rvqs.0.layers.5._codebook.cluster_size\", \"rq.rvqs.0.layers.5._codebook.embed_avg\", \"rq.rvqs.0.layers.5._codebook.embed\", \"rq.rvqs.0.layers.6._codebook.initted\", \"rq.rvqs.0.layers.6._codebook.cluster_size\", \"rq.rvqs.0.layers.6._codebook.embed_avg\", \"rq.rvqs.0.layers.6._codebook.embed\", \"rq.rvqs.0.layers.7._codebook.initted\", \"rq.rvqs.0.layers.7._codebook.cluster_size\", \"rq.rvqs.0.layers.7._codebook.embed_avg\", \"rq.rvqs.0.layers.7._codebook.embed\", \"decoder_film.to_cond.weight\", \"decoder_film.to_cond.bias\", \"decoder_attn.layers.0.0.q_scale\", \"decoder_attn.layers.0.0.k_scale\", \"decoder_attn.layers.0.0.norm.weight\", \"decoder_attn.layers.0.0.norm.bias\", \"decoder_attn.layers.0.0.to_qkv.weight\", \"decoder_attn.layers.0.0.attn_fn.rel_pos.inv_freq\", \"decoder_attn.layers.0.0.to_out.weight\", \"decoder_attn.layers.0.1.0.weight\", \"decoder_attn.layers.0.1.0.bias\", \"decoder_attn.layers.0.1.1.weight\", \"decoder_attn.layers.0.1.4.weight\", \"decoder.0.conv.weight\", \"decoder.0.conv.bias\", \"decoder.1.0.conv.weight\", \"decoder.1.0.conv.bias\", \"decoder.1.1.fn.0.conv.weight\", \"decoder.1.1.fn.0.conv.bias\", \"decoder.1.1.fn.2.conv.weight\", \"decoder.1.1.fn.2.conv.bias\", \"decoder.1.2.fn.0.conv.weight\", \"decoder.1.2.fn.0.conv.bias\", \"decoder.1.2.fn.2.conv.weight\", \"decoder.1.2.fn.2.conv.bias\", \"decoder.1.3.fn.0.conv.weight\", \"decoder.1.3.fn.0.conv.bias\", \"decoder.1.3.fn.2.conv.weight\", \"decoder.1.3.fn.2.conv.bias\", \"decoder.2.0.conv.weight\", \"decoder.2.0.conv.bias\", \"decoder.2.1.fn.0.conv.weight\", \"decoder.2.1.fn.0.conv.bias\", \"decoder.2.1.fn.2.conv.weight\", \"decoder.2.1.fn.2.conv.bias\", \"decoder.2.2.fn.0.conv.weight\", \"decoder.2.2.fn.0.conv.bias\", \"decoder.2.2.fn.2.conv.weight\", \"decoder.2.2.fn.2.conv.bias\", \"decoder.2.3.fn.0.conv.weight\", \"decoder.2.3.fn.0.conv.bias\", \"decoder.2.3.fn.2.conv.weight\", \"decoder.2.3.fn.2.conv.bias\", \"decoder.3.0.conv.weight\", \"decoder.3.0.conv.bias\", \"decoder.3.1.fn.0.conv.weight\", \"decoder.3.1.fn.0.conv.bias\", \"decoder.3.1.fn.2.conv.weight\", \"decoder.3.1.fn.2.conv.bias\", \"decoder.3.2.fn.0.conv.weight\", \"decoder.3.2.fn.0.conv.bias\", \"decoder.3.2.fn.2.conv.weight\", \"decoder.3.2.fn.2.conv.bias\", \"decoder.3.3.fn.0.conv.weight\", \"decoder.3.3.fn.0.conv.bias\", \"decoder.3.3.fn.2.conv.weight\", \"decoder.3.3.fn.2.conv.bias\", \"decoder.4.0.conv.weight\", \"decoder.4.0.conv.bias\", \"decoder.4.1.fn.0.conv.weight\", \"decoder.4.1.fn.0.conv.bias\", \"decoder.4.1.fn.2.conv.weight\", \"decoder.4.1.fn.2.conv.bias\", \"decoder.4.2.fn.0.conv.weight\", \"decoder.4.2.fn.0.conv.bias\", \"decoder.4.2.fn.2.conv.weight\", \"decoder.4.2.fn.2.conv.bias\", \"decoder.4.3.fn.0.conv.weight\", \"decoder.4.3.fn.0.conv.bias\", \"decoder.4.3.fn.2.conv.weight\", \"decoder.4.3.fn.2.conv.bias\", \"decoder.5.conv.weight\", \"decoder.5.conv.bias\", \"discriminators.0.init_conv.weight\", \"discriminators.0.init_conv.bias\", \"discriminators.0.conv_layers.0.0.weight\", \"discriminators.0.conv_layers.0.0.bias\", \"discriminators.0.conv_layers.1.0.weight\", \"discriminators.0.conv_layers.1.0.bias\", \"discriminators.0.conv_layers.2.0.weight\", \"discriminators.0.conv_layers.2.0.bias\", \"discriminators.0.conv_layers.3.0.weight\", \"discriminators.0.conv_layers.3.0.bias\", \"discriminators.0.final_conv.0.weight\", \"discriminators.0.final_conv.0.bias\", \"discriminators.0.final_conv.2.weight\", \"discriminators.0.final_conv.2.bias\", \"discriminators.1.init_conv.weight\", \"discriminators.1.init_conv.bias\", \"discriminators.1.conv_layers.0.0.weight\", \"discriminators.1.conv_layers.0.0.bias\", \"discriminators.1.conv_layers.1.0.weight\", \"discriminators.1.conv_layers.1.0.bias\", \"discriminators.1.conv_layers.2.0.weight\", \"discriminators.1.conv_layers.2.0.bias\", \"discriminators.1.conv_layers.3.0.weight\", \"discriminators.1.conv_layers.3.0.bias\", \"discriminators.1.final_conv.0.weight\", \"discriminators.1.final_conv.0.bias\", \"discriminators.1.final_conv.2.weight\", \"discriminators.1.final_conv.2.bias\", \"discriminators.2.init_conv.weight\", \"discriminators.2.init_conv.bias\", \"discriminators.2.conv_layers.0.0.weight\", \"discriminators.2.conv_layers.0.0.bias\", \"discriminators.2.conv_layers.1.0.weight\", \"discriminators.2.conv_layers.1.0.bias\", \"discriminators.2.conv_layers.2.0.weight\", \"discriminators.2.conv_layers.2.0.bias\", \"discriminators.2.conv_layers.3.0.weight\", \"discriminators.2.conv_layers.3.0.bias\", \"discriminators.2.final_conv.0.weight\", \"discriminators.2.final_conv.0.bias\", \"discriminators.2.final_conv.2.weight\", \"discriminators.2.final_conv.2.bias\", \"stft_discriminator.init_conv.weight\", \"stft_discriminator.init_conv.bias\", \"stft_discriminator.layers.0.0.fn.0.weight\", \"stft_discriminator.layers.0.0.fn.0.bias\", \"stft_discriminator.layers.0.0.fn.1.b\", \"stft_discriminator.layers.0.0.fn.2.weight\", \"stft_discriminator.layers.0.0.fn.2.bias\", \"stft_discriminator.layers.0.1.weight\", \"stft_discriminator.layers.0.1.bias\", \"stft_discriminator.layers.1.0.fn.0.weight\", \"stft_discriminator.layers.1.0.fn.0.bias\", \"stft_discriminator.layers.1.0.fn.1.b\", \"stft_discriminator.layers.1.0.fn.2.weight\", \"stft_discriminator.layers.1.0.fn.2.bias\", \"stft_discriminator.layers.1.1.weight\", \"stft_discriminator.layers.1.1.bias\", \"stft_discriminator.layers.2.0.fn.0.weight\", \"stft_discriminator.layers.2.0.fn.0.bias\", \"stft_discriminator.layers.2.0.fn.1.b\", \"stft_discriminator.layers.2.0.fn.2.weight\", \"stft_discriminator.layers.2.0.fn.2.bias\", \"stft_discriminator.layers.2.1.weight\", \"stft_discriminator.layers.2.1.bias\", \"stft_discriminator.layers.3.0.fn.0.weight\", \"stft_discriminator.layers.3.0.fn.0.bias\", \"stft_discriminator.layers.3.0.fn.1.b\", \"stft_discriminator.layers.3.0.fn.2.weight\", \"stft_discriminator.layers.3.0.fn.2.bias\", \"stft_discriminator.layers.3.1.weight\", \"stft_discriminator.layers.3.1.bias\", \"stft_discriminator.layers.4.0.fn.0.weight\", \"stft_discriminator.layers.4.0.fn.0.bias\", \"stft_discriminator.layers.4.0.fn.1.b\", \"stft_discriminator.layers.4.0.fn.2.weight\", \"stft_discriminator.layers.4.0.fn.2.bias\", \"stft_discriminator.layers.4.1.weight\", \"stft_discriminator.layers.4.1.bias\", \"stft_discriminator.layers.5.0.fn.0.weight\", \"stft_discriminator.layers.5.0.fn.0.bias\", \"stft_discriminator.layers.5.0.fn.1.b\", \"stft_discriminator.layers.5.0.fn.2.weight\", \"stft_discriminator.layers.5.0.fn.2.bias\", \"stft_discriminator.layers.5.1.weight\", \"stft_discriminator.layers.5.1.bias\", \"stft_discriminator.final_conv.weight\", \"stft_discriminator.final_conv.bias\", \"mel_spec_transforms.0.spectrogram.window\", \"mel_spec_transforms.0.mel_scale.fb\", \"mel_spec_transforms.1.spectrogram.window\", \"mel_spec_transforms.1.mel_scale.fb\", \"mel_spec_transforms.2.spectrogram.window\", \"mel_spec_transforms.2.mel_scale.fb\", \"mel_spec_transforms.3.spectrogram.window\", \"mel_spec_transforms.3.mel_scale.fb\", \"mel_spec_transforms.4.spectrogram.window\", \"mel_spec_transforms.4.mel_scale.fb\", \"mel_spec_transforms.5.spectrogram.window\", \"mel_spec_transforms.5.mel_scale.fb\". \n\tUnexpected key(s) in state_dict: \"start_token\", \"semantic_embedding.weight\", \"proj_text_embed.weight\", \"transformer.layers.0.0.norm.gamma\", \"transformer.layers.0.0.norm.beta\", \"transformer.layers.0.0.to_q.weight\", \"transformer.layers.0.0.to_kv.weight\", \"transformer.layers.0.0.to_out.0.weight\", \"transformer.layers.0.2.0.gamma\", \"transformer.layers.0.2.0.beta\", \"transformer.layers.0.2.1.weight\", \"transformer.layers.0.2.3.gamma\", \"transformer.layers.0.2.3.beta\", \"transformer.layers.0.2.5.weight\", \"transformer.layers.1.0.norm.gamma\", \"transformer.layers.1.0.norm.beta\", \"transformer.layers.1.0.to_q.weight\", \"transformer.layers.1.0.to_kv.weight\", \"transformer.layers.1.0.to_out.0.weight\", \"transformer.layers.1.2.0.gamma\", \"transformer.layers.1.2.0.beta\", \"transformer.layers.1.2.1.weight\", \"transformer.layers.1.2.3.gamma\", \"transformer.layers.1.2.3.beta\", \"transformer.layers.1.2.5.weight\", \"transformer.layers.2.0.norm.gamma\", \"transformer.layers.2.0.norm.beta\", \"transformer.layers.2.0.to_q.weight\", \"transformer.layers.2.0.to_kv.weight\", \"transformer.layers.2.0.to_out.0.weight\", \"transformer.layers.2.2.0.gamma\", \"transformer.layers.2.2.0.beta\", \"transformer.layers.2.2.1.weight\", \"transformer.layers.2.2.3.gamma\", \"transformer.layers.2.2.3.beta\", \"transformer.layers.2.2.5.weight\", \"transformer.layers.3.0.norm.gamma\", \"transformer.layers.3.0.norm.beta\", \"transformer.layers.3.0.to_q.weight\", \"transformer.layers.3.0.to_kv.weight\", \"transformer.layers.3.0.to_out.0.weight\", \"transformer.layers.3.2.0.gamma\", \"transformer.layers.3.2.0.beta\", \"transformer.layers.3.2.1.weight\", \"transformer.layers.3.2.3.gamma\", \"transformer.layers.3.2.3.beta\", \"transformer.layers.3.2.5.weight\", \"transformer.layers.4.0.norm.gamma\", \"transformer.layers.4.0.norm.beta\", \"transformer.layers.4.0.to_q.weight\", \"transformer.layers.4.0.to_kv.weight\", \"transformer.layers.4.0.to_out.0.weight\", \"transformer.layers.4.2.0.gamma\", \"transformer.layers.4.2.0.beta\", \"transformer.layers.4.2.1.weight\", \"transformer.layers.4.2.3.gamma\", \"transformer.layers.4.2.3.beta\", \"transformer.layers.4.2.5.weight\", \"transformer.layers.5.0.norm.gamma\", \"transformer.layers.5.0.norm.beta\", \"transformer.layers.5.0.to_q.weight\", \"transformer.layers.5.0.to_kv.weight\", \"transformer.layers.5.0.to_out.0.weight\", \"transformer.layers.5.2.0.gamma\", \"transformer.layers.5.2.0.beta\", \"transformer.layers.5.2.1.weight\", \"transformer.layers.5.2.3.gamma\", \"transformer.layers.5.2.3.beta\", \"transformer.layers.5.2.5.weight\", \"transformer.rel_pos_bias.net.0.0.weight\", \"transformer.rel_pos_bias.net.0.0.bias\", \"transformer.rel_pos_bias.net.1.0.weight\", \"transformer.rel_pos_bias.net.1.0.bias\", \"transformer.rel_pos_bias.net.2.0.weight\", \"transformer.rel_pos_bias.net.2.0.bias\", \"transformer.rel_pos_bias.net.3.weight\", \"transformer.rel_pos_bias.net.3.bias\", \"transformer.norm.gamma\", \"transformer.norm.beta\", \"to_logits.weight\", \"to_logits.bias\". "
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "fine_transformer = FineTransformer(\n",
    "    num_coarse_quantizers = 3,\n",
    "    num_fine_quantizers = 5,\n",
    "    codebook_size = 1024,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = FineTransformerTrainer(\n",
    "    transformer = fine_transformer,\n",
    "    codec = soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77c71c10-500a-4dba-971d-17a8a4c9c5af",
   "metadata": {},
   "outputs": [
    {
     "ename": "BeartypeCallHintParamViolation",
     "evalue": "Method audiolm_pytorch.audiolm_pytorch.AudioLM.__init__() parameter coarse_transformer=\"None\" violates type hint <class 'audiolm_pytorch.audiolm_pytorch.CoarseTransformer'>, as <class \"builtins.NoneType\"> \"None\" not instance of <class \"audiolm_pytorch.audiolm_pytorch.CoarseTransformer\">.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBeartypeCallHintParamViolation\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m fine_transformer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Everything together\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m audiolm \u001b[38;5;241m=\u001b[39m \u001b[43mAudioLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwav2vec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwav2vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msoundstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msemantic_transformer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msemantic_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoarse_transformer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoarse_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfine_transformer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfine_transformer\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m generated_wav \u001b[38;5;241m=\u001b[39m audiolm(batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m<@beartype(audiolm_pytorch.audiolm_pytorch.AudioLM.__init__) at 0x7f3a34382b00>:77\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_139887960904576, __beartype_object_139888001307840, __beartype_object_143379632, __beartype_object_143380592, __beartype_object_143381552, __beartype_object_139893706528000, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mBeartypeCallHintParamViolation\u001b[0m: Method audiolm_pytorch.audiolm_pytorch.AudioLM.__init__() parameter coarse_transformer=\"None\" violates type hint <class 'audiolm_pytorch.audiolm_pytorch.CoarseTransformer'>, as <class \"builtins.NoneType\"> \"None\" not instance of <class \"audiolm_pytorch.audiolm_pytorch.CoarseTransformer\">."
     ]
    }
   ],
   "source": [
    "coarse_transformer = None\n",
    "fine_transformer = None\n",
    "\n",
    "# Everything together\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = soundstream,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")\n",
    "\n",
    "generated_wav = audiolm(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea82d6-eb05-498b-93ed-16dd4461da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"out.wav\"\n",
    "sample_rate = 44100\n",
    "torchaudio.save(output_path, generated_wav.cpu(), sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
